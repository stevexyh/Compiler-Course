#!/usr/bin/env python3
# -*- coding: utf-8 -*-

'''
---------------------------------------------
* Project Name : Compiler-Course
* File Name    : lexical.py
* Description  : Lexer for the project
* Create Time  : 2020-05-28 02:07:25
* Version      : 1.0
* Author       : Steve X
* GitHub       : https://github.com/Steve-Xyh
---------------------------------------------
* Notice

- è¯æ³•åˆ†æçš„Token(ç»ˆç»“ç¬¦):
    * GE            å¤§äºç­‰äº >=
    * LE            å°äºç­‰äº <=
    * NE            ä¸ç­‰ç¬¦å· <>
    * AssignOper    èµ‹å€¼ç¬¦å· :=

    * IntNo         æ— ç¬¦å·æ•´æ•°
    * RealNo        æ— ç¬¦å·å®æ•°      å«ç§‘å­¦è®¡æ•°æ³•
    * Iden          å˜é‡æ ‡è¯†ç¬¦      ä»¥å­—æ¯æˆ–ä¸‹åˆ’çº¿å¼€å¤´ï¼Œåé¢ç´§è·Ÿå­—æ¯æˆ–æ•°å­—æˆ–ä¸‹åˆ’çº¿çš„å­—ç¬¦ä¸²

    * Program       å…³é”®å­— program
    * Var           å…³é”®å­— var
    * Begin         å…³é”®å­— begin
    * End           å…³é”®å­— end
    * Do            å…³é”®å­— do
    * While         å…³é”®å­— while
    * If            å…³é”®å­— if
    * Then          å…³é”®å­— then
    * Else          å…³é”®å­— else
    * And           å…³é”®å­— and      é€»è¾‘ä¸
    * Not           å…³é”®å­— not      é€»è¾‘é
    * Or            å…³é”®å­— or       é€»è¾‘æˆ–
    * Integer       å…³é”®å­— integer  æ•´æ•°ç±»å‹
    * Real          å…³é”®å­— reald    å®æ•°ç±»å‹

- è¿˜æœ‰æ–‡æ³•ä¸­çš„åˆ†éš”ç¬¦ã€ç®—ç¬¦ç­‰å­—ç¬¦å…³é”®å­—
---------------------------------------------
'''


# XXX(Steve X): è´Ÿæ•°è¿˜ä¸æ”¯æŒ, å½“æ—¶ç»™çš„æ–‡æ³•è¯æ³•é‡Œä¹Ÿæ²¡è¯´è¦è´Ÿæ•°å•Šhhhh, åªè¯´è¦æ— ç¬¦å·æ•°


import sys
import ply.lex as lex
import tools.format_string as fs
import tools.gen_table as gt
# if len(sys.argv) == 2:
INPUT_FILE = sys.argv[1]

# INPUT_FILE = 'input_pascal/addition.pas'
TABLE_LEN = 80

#---------------------------------Preset vars for PLY module---------------------------------#
reserved = {
    'program': 'Program',
    # 'Program': 'Program',
    'var': 'Var',
    # 'Var': 'Var',
    'begin': 'Begin',
    # 'Begin': 'Begin',
    'end': 'End',
    # 'End': 'End',
    'do': 'Do',
    # 'Do': 'Do',
    'while': 'While',
    # 'While': 'While',
    'if': 'If',
    # 'If': 'If',
    'then': 'Then',
    # 'Then': 'Then',
    'else': 'Else',
    # 'Else': 'Else',
    'and': 'And',
    # 'And': 'And',
    'not': 'Not',
    # 'Not': 'Not',
    'or': 'Or',
    # 'Or': 'Or',
    'integer': 'Integer',
    # 'Integer': 'Integer',

    # å·¦è¾¹åˆ°åº•æ˜¯ real è¿˜æ˜¯ reald, æ±Ÿä¿¡æ±Ÿç–‘ğŸ¤”
    'real': 'Real',
    # 'Reald': 'Real',

    'array': 'ARRAY',
    'of': 'OF',
}


# List of token names.   This is always required
tokens = [
    'GE',
    'LE',
    'NE',
    'AssignOper',
    'IntNo',
    'RealNo',
    'Iden',
] + list(reserved.values())

# Literal Characters
literals = "+-*/()<>[]=,;:."

# Regular expression rules for simple tokens
t_GE = r'>='
t_LE = r'<='
t_NE = r'<>'
t_AssignOper = r':='


# A string containing ignored characters (spaces and tabs)
t_ignore = ' \t'
#--------------------------------------------END---------------------------------------------#


#-----------------------------RegEx rules for preset functions-------------------------------#
def t_Iden(t):
    r'\b[a-zA-Z_][0-9a-zA-Z_]*'
    # r'\b[a-zA-Z_][0-9a-zA-Z_]*\b[^\.]'
    if t.value.endswith('\n'):
        t_newline(t)

    # Check for reserved words
    t.value = t.value.strip()
    t.type = reserved.get(t.value, 'Iden')
    return t


def t_RealNo(t):
    r'\b(\d*\.\d+|\d+(\.\d+)?([Ee][+-]?\d+))'
    # r'\b(\d*\.\d+|\d+(\.\d+)?([Ee][+-]?\d+))\b[^\.]'
    if t.value.endswith('\n'):
        t_newline(t)

    t.value = float(t.value)
    return t


def t_IntNo(t):
    r'\b[0-9]+'
    # r'\b[0-9]+\b[^\.]'
    if t.value.endswith('\n'):
        t_newline(t)

    t.value = int(t.value)
    return t


# Define a rule so we can track line numbers
def t_newline(t):
    r'\n'

    t.lexer.lineno += 1


def t_error(t):
    '''
    Error handling rule

    Parameters::
        t:LexToken - a token instance
    '''

    err_token = t.value.split()[0]
    column = find_column(input_str=INPUT_DATA, token=t)
    err_line = INPUT_DATA.splitlines()[t.lineno - 1]
    print('***')
    print(err_token)
    # Locate and mark the error
    fs.err_en({
        (' ' + str(t.lineno) + f' File "{INPUT_FILE}", line {t.lineno},col {column} '.ljust(35)): f" INVALID TOKEN '{err_token}'"
    })
    print(err_line)
    print(' '*(column - 1) + '='*len(err_token))
    print(' '*(column - 1) + 'â†‘')
    print('-'*TABLE_LEN)

    t.lexer.skip(len(err_token))


# Build the lexer
lexer = lex.lex(debug=True)
#--------------------------------------------END---------------------------------------------#


def find_column(input_str: str, token: lex.LexToken):
    '''
    Compute column.

    Parameters::
        input: str - the input text string
        token: LexToken - a token instance
    Returns::
        column: int - the column number of the token
    '''

    line_start = input_str.rfind('\n', 0, token.lexpos) + 1
    column = (token.lexpos - line_start) + 1
    return column


def read_data(file_name: str = INPUT_FILE):
    '''
    Read data from input Pascal file

    Parameters::
        file_name: str - name of the input file
    Returns::
        data: str - data read from the file
    '''

    data = ''
    with open(file=file_name, mode='r') as input_file:
        data = input_file.read().lower()

    return data


def buile_lines(data: str = ''):
    '''
    Input pascal file and run test
    Classify valid tokens by line number

    Parameters::
        data: str - the input string
    Returns::
        lines: list - a sorted list of tokens
    '''

    lexer.input(data)

    line_no = 0
    lines = [[{'line_num': line_no}]]
    for tok in lexer:
        while True:
            if tok.lineno == line_no:
                lines[line_no].append(tok)
                break
            else:
                line_no += 1
                lines.append([])
                lines[0][0]['line_num'] = line_no

        print(tok.lineno, tok.type, tok.value)

    print('#EOF')
    return lines


def output_table():
    '''
    Output the token table

    '''

    header = [
        {'header': 'Ln', 'justify': 'right', 'style': 'green'},
        {'header': 'Type', 'justify': 'left', 'style': 'cyan'},
        {'header': 'Value', 'justify': 'left', 'style': 'red'},
    ]

    data = []
    for row in LINE_LIST[1:]:
        for tok in row:
            data.append((str(tok.lineno), tok.type, str(tok.value)))

    gt.print_table(header_list=header, data_list=data)


# if __name__ == "__main__":

INPUT_DATA = read_data(file_name=INPUT_FILE)
LINE_LIST = buile_lines(data=INPUT_DATA)
# for l in LINE_LIST:
#     print(l)
# print(LINE_LIST[0][0]['line_num'])
output_table()
